# -*- coding: utf-8 -*-
"""Success_hms-v3_KLD-TF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_caaBW0uygl4l2MyU8MysyTRQf37mRRs

# HMS - Harmful Brain Activity Classification

### IMPORTING THE NECESSARY LIBRARIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

import warnings
warnings.filterwarnings('ignore')

"""## LOADING THE DATASET"""

BASE_DIR = "/home/m1/23CS60R76/MTP_Works/HMS_Brain_Activity/Harmful_Brain_Activity/"

df = pd.read_csv(f"{BASE_DIR}train.csv")
df.head()

df["total_votes"] = df[["seizure_vote","lpd_vote","gpd_vote","lrda_vote","grda_vote","other_vote"]].sum(axis=1)
print("Total unique votes:", df["total_votes"].unique())

EEGid_label_list = df[["eeg_id","eeg_label_offset_seconds","seizure_vote","lpd_vote","gpd_vote","lrda_vote","grda_vote","other_vote","total_votes"]].values.tolist()
print("Total Samples:", len(EEGid_label_list))

"""## MODEL CREATION"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

def data_generator(X, y, batch_size=32):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)
    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

class CustomDataGen(tf.keras.utils.Sequence):
    
    def __init__(self, EEGid_label_list,
                 batch_size,
                 shuffle=True):
        
        self.batch_size = batch_size
        self.shuffle = shuffle
        
        self.n = len(EEGid_label_list)
        self.n_name = df[y_col['name']].nunique()
        self.n_type = df[y_col['type']].nunique()
    
    def on_epoch_end(self):
        if self.shuffle:
            self.df = self.df.sample(frac=1).reset_index(drop=True)
    
    def __get_input(self, path, bbox, target_size):
    
        xmin, ymin, w, h = bbox['x'], bbox['y'], bbox['width'], bbox['height']

        image = tf.keras.preprocessing.image.load_img(path)
        image_arr = tf.keras.preprocessing.image.img_to_array(image)

        image_arr = image_arr[ymin:ymin+h, xmin:xmin+w]
        image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()

        return image_arr/255.
    
    def __get_output(self, label, num_classes):
        return tf.keras.utils.to_categorical(label, num_classes=num_classes)
    
    def __get_data(self, batches):
        # Generates data containing batch_size samples

        path_batch = batches[self.X_col['path']]
        bbox_batch = batches[self.X_col['bbox']]
        
        name_batch = batches[self.y_col['name']]
        type_batch = batches[self.y_col['type']]

        X_batch = np.asarray([self.__get_input(x, y, self.input_size) for x, y in zip(path_batch, bbox_batch)])

        y0_batch = np.asarray([self.__get_output(y, self.n_name) for y in name_batch])
        y1_batch = np.asarray([self.__get_output(y, self.n_type) for y in type_batch])

        return X_batch, tuple([y0_batch, y1_batch])
    
    def __getitem__(self, index):
        
        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]
        X, y = self.__get_data(batches)        
        return X, y
    
    def __len__(self):
        return self.n // self.batch_size

def createDataset(EEGid_label_list, index_list):
    X = []; y = []
    prev_eegId = ""; idx=0
    for x in EEGid_label_list:
        eeg_id, offset, sv, lpv, gpv, lrv, grv, ov, tv = x
        eeg_id = int(eeg_id)
        if(eeg_id!=prev_eegId):
            temp_df = pd.read_parquet(f'{BASE_DIR}train_eegs/{eeg_id}.parquet')
            C = ['Fp1', 'T3', 'P4']
            temp_arr = temp_df[C].to_numpy().T
            temp_arr[np.isnan(temp_arr)] = 0
        start = 200*int(offset)
        if(idx in index_list):
            X.append(temp_arr[:, start:start+10000])
            votes_arr = np.array([sv, lpv, gpv, lrv, grv, ov])
            y.append(votes_arr/tv)
        prev_eegId = eeg_id
        idx+=1
    X = np.array(X)
    y = np.array(y)
    print("X:", X.shape)
    print("y:", y.shape)
    # CNN will be used. Channel dimension is added.
    X = X[:, :, :, np.newaxis]
    return X,y

## deep learning model
def createModel(ip_shape):
    saved_model_path = '/home/m1/23CS60R76/MTP_Works/BestSavedModels/HMS_model7_v1.keras'

    if(os.path.isfile(saved_model_path)):
        model = keras.models.load_model(saved_model_path)
        print("Model Loaded :", saved_model_path)
    else:
        model = keras.models.Sequential()

        model.add(layers.Conv2D(filters=64, kernel_size=(2, 4), padding='same', activation='relu', input_shape=ip_shape))
        model.add(layers.Conv2D(filters=64, kernel_size=(2, 4), strides=(1, 2),padding='same', activation='relu'))
        model.add(layers.MaxPooling2D((1, 2)))

        model.add(layers.Conv2D(filters=128, kernel_size=(2, 4), padding='same', activation='relu'))
        model.add(layers.Conv2D(filters=128, kernel_size=(2, 4), strides=(1, 2), padding='same', activation='relu'))
        model.add(layers.MaxPooling2D((2, 2)))

        model.add(layers.Conv2D(filters=256, kernel_size=(4, 4), padding='same', activation='relu'))
        model.add(layers.Conv2D(filters=256, kernel_size=(4, 4), strides=(1, 2), padding='same', activation='relu'))
        model.add(layers.MaxPooling2D((1, 2)))

        model.add(layers.GlobalAveragePooling2D())
        #model.add(layers.Flatten())
        model.add(layers.Dense(256, activation='relu'))
        model.add(layers.Dropout(0.25))
        model.add(layers.Dense(128, activation='relu'))
        model.add(layers.Dense(64, activation='relu'))
        model.add(layers.Dropout(0.25))
        model.add(layers.Dense(6, activation='softmax'))
        print("Model Created.")
    LEARNING_RATE = 1e-5
    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    model.compile(optimizer=OPTIMIZER, loss='kl_divergence', metrics=['accuracy'])
    return model

indices = list(range(len(EEGid_label_list)))
train_indices, val_indices = train_test_split(indices, test_size=0.3, random_state=42)
# val_indices, test_indices = train_test_split(val_indices, test_size=0.5, random_state=42)

print("\n--Training:")
X_train, y_train = createDataset(EEGid_label_list, train_indices)
# train_dataset = data_generator(X_train, y_train, batch_size=64)

print("\n--Validation:")
X_val, y_val = createDataset(EEGid_label_list, val_indices)
# val_dataset = data_generator(X_val, y_val, batch_size=64)

model = createModel((3,10000,1))
model.summary()

# callbacks
VERBOSE=0
#lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=5, verbose=VERBOSE, min_le=1e-8)
es = EarlyStopping(monitor='val_loss', patience=50, verbose=VERBOSE, mode='auto', restore_best_weights=True)
checkpoint_filepath = '/home/m1/23CS60R76/MTP_Works/BestSavedModels/HMS_model7_v1.keras'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True)
callbacks = [es, model_checkpoint_callback]
hist = model.fit(
    x=X_train, y=y_train,
    validation_data=(X_val,y_val),
    epochs=250,
    batch_size=64,
    callbacks=callbacks
)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Saving the figure.
plt.savefig("training_history-3.jpg")